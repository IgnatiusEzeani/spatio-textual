{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ET6wKFGP-SOG",
        "Kt1OnOCpD13Q",
        "COQgoH2iLOpz",
        "ApoODrTs87P8"
      ],
      "authorship_tag": "ABX9TyN+IyQw1XhvFuqmSGjmFEnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnatiusEzeani/spatio-textual/blob/main/spatio_textual_package_a_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing `spatio-textual` Python package"
      ],
      "metadata": {
        "id": "QBrCPj878Txz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spatio-textual**: a Python package for spatial entity recognition and verb relation extraction from text created by the [Spatial Narratives Project](https://spacetimenarratives.github.io/) and designed to support spatio-textual annotation, analysis and visualization in digital humanities projects, with initial applications to:\n",
        "\n",
        "- the *Corpus of Lake District Writing* (CLDW)\n",
        "- Holocaust survivors' testimonies (e.g., USC Shoah Foundation archives)\n",
        "\n",
        "This package leverages spaCy and gazetteer-based classification to identify and label spatial entities such as cities, countries, camps, and geographic nouns, and also extracts action-verb contexts involving these entities.\n"
      ],
      "metadata": {
        "id": "8OL1Nd-F8WEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Setting up\n",
        "Download `en_core_web_trf` spaCy model and install `spatio-textual` package.\n",
        "\n",
        "**_Note:_** *Please wait a while, this may take a minute or 2...* üïê\n"
      ],
      "metadata": {
        "id": "ET6wKFGP-SOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf\n",
        "!pip install -q git+https://github.com/SpaceTimeNarratives/spatio-textual.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuaB_QNJcoYc",
        "outputId": "c61c1e45-cad7-462f-e3e1-c292c576b359"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from en-core-web-trf==3.8.0) (0.3.1)\n",
            "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.1.1)\n",
            "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.0.9)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.12/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.2)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Importing the `spatio-textual` package\n",
        "Having successfully downloaded the spaCy model and installed the `spatio-textual` package, it can now be imported and used in a Python environment to process text.\n",
        "\n",
        "*Again, this may take about a minute too, sorry...*"
      ],
      "metadata": {
        "id": "Kt1OnOCpD13Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1PJGz3edrWZR"
      },
      "outputs": [],
      "source": [
        "import spatio_textual"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `spatio-textual` package has the `annotate` module with functions `annotate_text`, `annotate_texts`, `chunk_and_annotate_text`, `chunk_and_annotate_file` which identifies and labels spatial entities in text inputs of different formats.\n",
        "\n",
        "So we can import the functions directly as below"
      ],
      "metadata": {
        "id": "x2PfuuwaJcFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spatio_textual.annotate import (\n",
        "    annotate_text,            # annotates a single text\n",
        "    annotate_texts,           # annotates a collection of texts\n",
        "    chunk_and_annotate_text,  # chunks and annotates a text\n",
        "    chunk_and_annotate_file,  # chunks and annotates a file\n",
        ")"
      ],
      "metadata": {
        "id": "5zd_SQRQ8fSo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Annotating spatial entities"
      ],
      "metadata": {
        "id": "COQgoH2iLOpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beyond the typical labels for the named entity recognition task [`PERSON`, `ORG`, `LOC`, `DATE`], we have defined a set of entity labels that are relevant for our work as shown below:\n",
        "\n",
        "| Tag          | Description                                                  |\n",
        "| ------------ | ------------------------------------------------------------ |\n",
        "| `PERSON`     | A named person                                               |\n",
        "| `CONTINENT`  | A continent name (e.g. ‚ÄúEurope‚Äù, ‚ÄúAsia‚Äù)                     |\n",
        "| `COUNTRY`    | A country name (e.g. ‚ÄúGermany‚Äù, ‚ÄúCzechoslovakia‚Äù)            |\n",
        "| `US-STATE`   | A U.S. state name (e.g. ‚ÄúCalifornia‚Äù, ‚ÄúNew York‚Äù)            |\n",
        "| `CITY`       | A city name (e.g. ‚ÄúBerlin‚Äù, ‚ÄúLondon‚Äù,  when classified)     |\n",
        "| `CAMP`       | A Holocaust-camp name e.g. ‚ÄúAuschwitz‚Äù (from your custom list)                |\n",
        "| `PLACE`      | Other place-type entities not matched above                  |\n",
        "| `GEONOUN`    | Generic geographic nouns (e.g. ‚Äúvalley‚Äù, ‚Äúmoor‚Äù)             |\n",
        "| `NON-VERBAL` | Terms like [PAUSES], [LAUGHS] in non-verbal list |\n",
        "| `FAMILY`     | Kinship terms (e.g. ‚Äúmother‚Äù, ‚Äúuncle‚Äù)                       |\n",
        "| `DATE`       | Temporal expressions (e.g. ‚ÄúMarch 9, 1996‚Äù)                  |\n",
        "| `TIME`       | Time-of-day expressions (e.g. ‚Äú3 PM‚Äù)                        |\n",
        "| `EVENT`      | Named events (e.g. ‚ÄúD-Day‚Äù)                                  |\n",
        "| `QUANTITY`   | Numeric/measure expressions (e.g. ‚Äú100 miles‚Äù)               |\n",
        "\n",
        "We will demonstrate how to use the `annotate` module functions to label spatial entities in text in the next cells."
      ],
      "metadata": {
        "id": "ErwJyCwoOn9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotating text: `annotate_text(...)`"
      ],
      "metadata": {
        "id": "ApoODrTs87P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###### Assume we have our text stored in `text` variable as below...\n",
        "text = \"\"\"During the summer of 1942, my family and I were deported from our home in Krakow to the Plaszow labor camp.\n",
        "We spent several difficult months there before being transferred to Auschwitz-Birkenau.\"\n",
        "\"\"\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "NxjAfBllvcoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###### ...we can then identify the entities with the `annotate_text(...)` function.\n",
        "result = annotate_text(text)\n",
        "result"
      ],
      "metadata": {
        "id": "PK-CFbiTsUx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "e4f166de-65cb-4413-b70f-e819f1d1c02c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'entities': [{'start_char': 9, 'token': 'the summer of 1942', 'tag': 'DATE'},\n",
              "  {'start_char': 76, 'token': 'Krakow', 'tag': 'PLACE'},\n",
              "  {'start_char': 90, 'token': 'Plaszow', 'tag': 'PLACE'},\n",
              "  {'start_char': 98, 'token': 'labor camp', 'tag': 'GEONOUN'},\n",
              "  {'start_char': 119, 'token': 'several difficult months', 'tag': 'DATE'},\n",
              "  {'start_char': 178, 'token': 'Auschwitz-Birkenau', 'tag': 'CAMP'}],\n",
              " 'verb_data': []}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see tokens identified as places (`PLACE`), geonouns (`GEONOUN`), camps (`CAMP`) etc.\n",
        "\n",
        "Observe, however, that the `verb_data` value is empty. The `verb_data` is meant to capture the 'actions' performed by actors in the text for further analysis.\n",
        "\n",
        "To fix this, we can set the optional parameter `include-verbs` to `True` in the function call."
      ],
      "metadata": {
        "id": "-4ZXu8eS4ZY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###### So let's modify the function calls to extract `verb_data`\n",
        "result = annotate_text(text, include_verbs=True)\n",
        "\n",
        "print(\"Entities:\")\n",
        "display(result['entities'])\n",
        "\n",
        "print(\"\\nVerb Data:\")\n",
        "display(result['verb_data'])"
      ],
      "metadata": {
        "id": "CS4v-HoKWrC0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "14aae2c6-a37a-427e-e4bf-7091b35a0c36",
        "cellView": "form"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'start_char': 9, 'token': 'the summer of 1942', 'tag': 'DATE'},\n",
              " {'start_char': 76, 'token': 'Krakow', 'tag': 'PLACE'},\n",
              " {'start_char': 90, 'token': 'Plaszow', 'tag': 'PLACE'},\n",
              " {'start_char': 98, 'token': 'labor camp', 'tag': 'GEONOUN'},\n",
              " {'start_char': 119, 'token': 'several difficult months', 'tag': 'DATE'},\n",
              " {'start_char': 178, 'token': 'Auschwitz-Birkenau', 'tag': 'CAMP'}]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verb Data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'sent-id': 0,\n",
              "  'verb': 'deported',\n",
              "  'subject': 'family',\n",
              "  'object': 'the summer of 1942',\n",
              "  'sentence': '\\n\"During the summer of 1942, my family and I were deported from our home in Krakow to the Plaszow labor camp.'},\n",
              " {'sent-id': 1,\n",
              "  'verb': 'spent',\n",
              "  'subject': 'We',\n",
              "  'object': 'several difficult months',\n",
              "  'sentence': '\\nWe spent several difficult months there before being transferred to Auschwitz-Birkenau.\"'},\n",
              " {'sent-id': 1,\n",
              "  'verb': 'transferred',\n",
              "  'subject': '',\n",
              "  'object': 'Auschwitz-Birkenau',\n",
              "  'sentence': '\\nWe spent several difficult months there before being transferred to Auschwitz-Birkenau.\"'}]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotating a list of texts"
      ],
      "metadata": {
        "id": "v-NkvcFLE8H2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have a collection of texts (instead of just one piece of text), we can use the `annotate_texts(...)` instead."
      ],
      "metadata": {
        "id": "WaGV2fSNBd7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_texts = [\n",
        "    \"My family and I were deported from our home in Krakow to the Plaszow labor camp.\",\n",
        "    \"We spent several difficult months there before being transferred to Maribor.\",\n",
        "    \"Finally, we arrived at a place with barbed wire fences and watchtowers.\",\n",
        "    \"It was Auschwitz.\"\n",
        "]\n",
        "\n",
        "results = annotate_texts(list_of_texts, include_verbs=True)\n",
        "for result in results:\n",
        "    print(\"\\nEntities:\")\n",
        "    display(result['entities'])\n",
        "\n",
        "    print(\"\\nVerb Data:\")\n",
        "    display(result['verb_data'])"
      ],
      "metadata": {
        "id": "3_pMriYiCTgx",
        "outputId": "0dbc17a5-89e2-410f-c39c-dd9fd1f5bb6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entities:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'start_char': 47, 'token': 'Krakow', 'tag': 'PLACE'},\n",
              " {'start_char': 61, 'token': 'Plaszow', 'tag': 'PLACE'},\n",
              " {'start_char': 69, 'token': 'labor camp', 'tag': 'GEONOUN'}]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verb Data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'sent-id': 0,\n",
              "  'verb': 'deported',\n",
              "  'subject': 'family',\n",
              "  'object': 'home',\n",
              "  'sentence': 'My family and I were deported from our home in Krakow to the Plaszow labor camp.'}]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entities:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'start_char': 9, 'token': 'several difficult months', 'tag': 'DATE'},\n",
              " {'start_char': 68, 'token': 'Maribor', 'tag': 'CITY'},\n",
              " {'start_char': 136, 'token': 'watchtowers', 'tag': 'GEONOUN'},\n",
              " {'start_char': 156, 'token': 'Auschwitz', 'tag': 'CAMP'}]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verb Data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'sent-id': 0,\n",
              "  'verb': 'spent',\n",
              "  'subject': 'We',\n",
              "  'object': 'several difficult months',\n",
              "  'sentence': 'We spent several difficult months there before being transferred to Maribor.'},\n",
              " {'sent-id': 0,\n",
              "  'verb': 'transferred',\n",
              "  'subject': '',\n",
              "  'object': 'Maribor',\n",
              "  'sentence': 'We spent several difficult months there before being transferred to Maribor.'},\n",
              " {'sent-id': 1,\n",
              "  'verb': 'arrived',\n",
              "  'subject': 'we',\n",
              "  'object': 'place',\n",
              "  'sentence': 'Finally, we arrived at a place with barbed wire fences and watchtowers.'}]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotating text segments"
      ],
      "metadata": {
        "id": "OzoBuJOKjsiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, we may need to split a text into segments (or chunks) before annotating them. This is similar to annotating a list of texts only that it includes the segmentation feature.\n",
        "\n",
        "This can be achieved by using `chunk_and_annotate_text(...)` on a text string or using `chunk_and_annotate_file(...)` on a text file. In both cases a key parameter to set is the `n_segments` which specifies the number of segments."
      ],
      "metadata": {
        "id": "obElrU2hj53B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using `chunk_and_annotate_text(...)`"
      ],
      "metadata": {
        "id": "kdUT5Q4Nv4EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### To demonstrate this let's download, read and display the text file `long-text`...\n",
        "!wget -c -q \"https://raw.githubusercontent.com/SpaceTimeNarratives/spatio-textual/refs/heads/main/example-texts/long-text\"\n",
        "text = open(\"long-text\", 'r').read()\n",
        "display(text)"
      ],
      "metadata": {
        "id": "pA_j2RQ0jrzx",
        "outputId": "f1ddb661-a388-484d-a70a-27babcce6e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"It was the spring of 1943 when they came for us. We were living in a small village near Krakow. The soldiers arrived early in the morning, shouting. They rounded everyone up and forced us onto crowded trains. The journey was terrible, days and nights without food or water. We didn't know where we were going, but the fear was palpable.\\n\\nFinally, we arrived at a place with barbed wire fences and watchtowers. It was Auschwitz. The separation was immediate. Men to one side, women and children to the other. I never saw my father again. My mother and I were sent to the women's camp. The conditions were horrific. Overcrowding, starvation, disease. We were forced to do hard labor, building roads and clearing rubble.\\n\\nOne day, my mother became very ill. I tried to care for her, but there was nothing I could do. She died a few weeks later. I was alone. The days blurred into weeks, weeks into months. The constant threat of death was always present. Selections were frequent, and those deemed too weak to work were sent to the gas chambers. I don't know how I survived. Maybe it was luck, maybe it was sheer will. The memories are still vivid, even after all these years. The hunger, the cold, the constant fear, the loss of everything I ever knew. It's a burden I carry every day.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### ... then let's segment and annotate it with `chunk_and_annotate_text(...)`\n",
        "result = chunk_and_annotate_text(text, n_segments=5, include_text=True)\n",
        "result"
      ],
      "metadata": {
        "id": "P4jygm2Gl4Y8",
        "outputId": "7db4738d-aab3-41d1-deee-63a719d56f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entities': [{'start_char': 11, 'token': 'spring', 'tag': 'GEONOUN'},\n",
              "   {'start_char': 75, 'token': 'village', 'tag': 'GEONOUN'},\n",
              "   {'start_char': 88, 'token': 'Krakow', 'tag': 'PLACE'},\n",
              "   {'start_char': 117, 'token': 'early in the morning', 'tag': 'TIME'},\n",
              "   {'start_char': 235, 'token': 'days', 'tag': 'DATE'},\n",
              "   {'start_char': 244, 'token': 'nights', 'tag': 'DATE'}],\n",
              "  'verb_data': [],\n",
              "  'segId': 1,\n",
              "  'text': \"It was the spring of 1943 when they came for us. We were living in a small village near Krakow. The soldiers arrived early in the morning, shouting. They rounded everyone up and forced us onto crowded trains. The journey was terrible, days and nights without food or water. We didn't know where we were going, but the fear was palpable.\"},\n",
              " {'entities': [{'start_char': 59, 'token': 'watchtowers', 'tag': 'GEONOUN'},\n",
              "   {'start_char': 79, 'token': 'Auschwitz', 'tag': 'CAMP'},\n",
              "   {'start_char': 147, 'token': 'children', 'tag': 'FAMILY'},\n",
              "   {'start_char': 185, 'token': 'father', 'tag': 'FAMILY'},\n",
              "   {'start_char': 202, 'token': 'mother', 'tag': 'FAMILY'},\n",
              "   {'start_char': 240, 'token': 'camp', 'tag': 'GEONOUN'}],\n",
              "  'verb_data': [],\n",
              "  'segId': 2,\n",
              "  'text': \"Finally, we arrived at a place with barbed wire fences and watchtowers. It was Auschwitz. The separation was immediate. Men to one side, women and children to the other. I never saw my father again. My mother and I were sent to the women's camp.\"},\n",
              " {'entities': [{'start_char': 98, 'token': 'building', 'tag': 'GEONOUN'},\n",
              "   {'start_char': 146, 'token': 'mother', 'tag': 'FAMILY'}],\n",
              "  'verb_data': [],\n",
              "  'segId': 3,\n",
              "  'text': 'The conditions were horrific. Overcrowding, starvation, disease. We were forced to do hard labor, building roads and clearing rubble. One day, my mother became very ill. I tried to care for her, but there was nothing I could do.'},\n",
              " {'entities': [{'start_char': 9, 'token': 'a few weeks later', 'tag': 'DATE'},\n",
              "   {'start_char': 41, 'token': 'The days', 'tag': 'DATE'},\n",
              "   {'start_char': 63, 'token': 'weeks', 'tag': 'DATE'},\n",
              "   {'start_char': 70, 'token': 'weeks', 'tag': 'DATE'},\n",
              "   {'start_char': 81, 'token': 'months', 'tag': 'DATE'}],\n",
              "  'verb_data': [],\n",
              "  'segId': 4,\n",
              "  'text': 'She died a few weeks later. I was alone. The days blurred into weeks, weeks into months. The constant threat of death was always present. Selections were frequent, and those deemed too weak to work were sent to the gas chambers.'},\n",
              " {'entities': [{'start_char': 124, 'token': 'years', 'tag': 'DATE'},\n",
              "   {'start_char': 230, 'token': 'every day', 'tag': 'DATE'}],\n",
              "  'verb_data': [],\n",
              "  'segId': 5,\n",
              "  'text': \"I don't know how I survived. Maybe it was luck, maybe it was sheer will. The memories are still vivid, even after all these years. The hunger, the cold, the constant fear, the loss of everything I ever knew. It's a burden I carry every day.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using `chunk_and_annotate_file(...)`"
      ],
      "metadata": {
        "id": "T7PtumbGwJW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**NOTE:** You can also upload your file using the file icon in the left sidebar. Then, replace the example text `long-text` with the name of your uploaded file."
      ],
      "metadata": {
        "id": "iY0BiQkrcRVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### ... Let's segment and annotate a file `chunk_and_annotate_file(...)`\n",
        "result = chunk_and_annotate_file('long-text', n_segments=5)\n",
        "result"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NzmT58ASwKP8",
        "outputId": "732b12ec-0213-433f-a479-ac5ebcf45fbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entities': [{'start_char': 11, 'token': 'spring', 'tag': 'GEONOUN'},\n",
              "   {'start_char': 75, 'token': 'village', 'tag': 'GEONOUN'},\n",
              "   {'start_char': 88, 'token': 'Krakow', 'tag': 'PLACE'},\n",
              "   {'start_char': 117, 'token': 'early in the morning', 'tag': 'TIME'},\n",
              "   {'start_char': 235, 'token': 'days', 'tag': 'DATE'},\n",
              "   {'start_char': 244, 'token': 'nights', 'tag': 'DATE'}],\n",
              "  'verb_data': [],\n",
              "  'file': 'long-text',\n",
              "  'fileId': 'long-text',\n",
              "  'segId': 1,\n",
              "  'segCount': 5},\n",
              " {'entities': [{'start_char': 59, 'token': 'watchtowers', 'tag': 'GEONOUN'},\n",
              "   {'start_char': 79, 'token': 'Auschwitz', 'tag': 'CAMP'},\n",
              "   {'start_char': 147, 'token': 'children', 'tag': 'FAMILY'},\n",
              "   {'start_char': 185, 'token': 'father', 'tag': 'FAMILY'},\n",
              "   {'start_char': 202, 'token': 'mother', 'tag': 'FAMILY'},\n",
              "   {'start_char': 240, 'token': 'camp', 'tag': 'GEONOUN'}],\n",
              "  'verb_data': [],\n",
              "  'file': 'long-text',\n",
              "  'fileId': 'long-text',\n",
              "  'segId': 2,\n",
              "  'segCount': 5},\n",
              " {'entities': [{'start_char': 98, 'token': 'building', 'tag': 'GEONOUN'},\n",
              "   {'start_char': 146, 'token': 'mother', 'tag': 'FAMILY'}],\n",
              "  'verb_data': [],\n",
              "  'file': 'long-text',\n",
              "  'fileId': 'long-text',\n",
              "  'segId': 3,\n",
              "  'segCount': 5},\n",
              " {'entities': [{'start_char': 9, 'token': 'a few weeks later', 'tag': 'DATE'},\n",
              "   {'start_char': 41, 'token': 'The days', 'tag': 'DATE'},\n",
              "   {'start_char': 63, 'token': 'weeks', 'tag': 'DATE'},\n",
              "   {'start_char': 70, 'token': 'weeks', 'tag': 'DATE'},\n",
              "   {'start_char': 81, 'token': 'months', 'tag': 'DATE'}],\n",
              "  'verb_data': [],\n",
              "  'file': 'long-text',\n",
              "  'fileId': 'long-text',\n",
              "  'segId': 4,\n",
              "  'segCount': 5},\n",
              " {'entities': [{'start_char': 124, 'token': 'years', 'tag': 'DATE'},\n",
              "   {'start_char': 230, 'token': 'every day', 'tag': 'DATE'}],\n",
              "  'verb_data': [],\n",
              "  'file': 'long-text',\n",
              "  'fileId': 'long-text',\n",
              "  'segId': 5,\n",
              "  'segCount': 5}]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotating files"
      ],
      "metadata": {
        "id": "ZRduMzA98pBR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "456eaef0"
      },
      "source": [
        "We can also pass a file or a collection of files in a folder as an input to annotate,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = annotate_files(['./*'], chunk=False, include_text=True, include_verbs=True)"
      ],
      "metadata": {
        "id": "v64g-VB0zzdE"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from spatio_textual.utils import save_annotations\n",
        "save_annotations(result, 'result.jsonl')"
      ],
      "metadata": {
        "id": "xENxoDoW1taO"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ToDo\n",
        "Additional features to include\n",
        "- Annotating a list of texts\n",
        "- Saving annotations to files (what format: csv, json, txt)\n",
        "- Emotion classification (LLM vs BERT-based)\n",
        "- Sentiment.\n",
        "- Geocoding\n",
        "- Create the `Lake District` and `Holocaust` modules?\n",
        "  - `Holocaust`:\n",
        "      - `journey` extraction\n",
        "      - `event` extraction\n",
        "  - `Lake District`:\n",
        "      - `nearness`\n",
        "      - `wild` and `picturesque`\n",
        "- Analysis\n",
        "- Visualization"
      ],
      "metadata": {
        "id": "Lda1ctcazsUL"
      }
    }
  ]
}