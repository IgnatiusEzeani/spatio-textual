{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ET6wKFGP-SOG",
        "Kt1OnOCpD13Q",
        "COQgoH2iLOpz",
        "ApoODrTs87P8"
      ],
      "authorship_tag": "ABX9TyPj0c/DEd44b9O3SuBDOFV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnatiusEzeani/spatio-textual/blob/main/spatio_textual_package_a_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing `spatio-textual` Python package"
      ],
      "metadata": {
        "id": "QBrCPj878Txz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spatio-textual**: a Python package for spatial entity recognition and verb relation extraction from text created by the [Spatial Narratives Project](https://spacetimenarratives.github.io/) and designed to support spatio-textual annotation, analysis and visualization in digital humanities projects, with initial applications to:\n",
        "\n",
        "- the *Corpus of Lake District Writing* (CLDW)\n",
        "- Holocaust survivors' testimonies (e.g., USC Shoah Foundation archives)\n",
        "\n",
        "This package leverages spaCy and gazetteer-based classification to identify and label spatial entities such as cities, countries, camps, and geographic nouns, and also extracts action-verb contexts involving these entities.\n"
      ],
      "metadata": {
        "id": "8OL1Nd-F8WEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Setting up\n",
        "Download `en_core_web_trf` spaCy model and install `spatio-textual` package.\n",
        "\n",
        "**_Note:_** *Please wait a while, this may take a minute or 2...* üïê\n"
      ],
      "metadata": {
        "id": "ET6wKFGP-SOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf\n",
        "!pip install -q git+https://github.com/SpaceTimeNarratives/spatio-textual.git"
      ],
      "metadata": {
        "id": "FuaB_QNJcoYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Importing the `spatio-textual` package\n",
        "Having successfully downloaded the spaCy model and installed the `spatio-textual` package, it can now be imported and used in a Python environment to process text.\n",
        "\n",
        "*Again, this may take about a minute too, sorry...*"
      ],
      "metadata": {
        "id": "Kt1OnOCpD13Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PJGz3edrWZR"
      },
      "outputs": [],
      "source": [
        "import spatio_textual"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `spatio-textual` package has the `annotate` module with functions `annotate_text`, `annotate_texts`, `chunk_and_annotate_text`, `chunk_and_annotate_file` which identifies and labels spatial entities in text inputs of different formats.\n",
        "\n",
        "So we can import the functions directly as below"
      ],
      "metadata": {
        "id": "x2PfuuwaJcFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spatio_textual.annotate import (\n",
        "    annotate_text,            # annotates a single text\n",
        "    annotate_texts,           # annotates a collection of texts\n",
        "    chunk_and_annotate_text,  # chunks and annotates a text\n",
        "    chunk_and_annotate_file,  # chunks and annotates a file\n",
        "    annotate_files,           # annotates a collection of files\n",
        ")"
      ],
      "metadata": {
        "id": "5zd_SQRQ8fSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we can manage the saving and loading of annotations using the package `utils` functions `save_annotations(...)` and `load_annotations(...)` respectively"
      ],
      "metadata": {
        "id": "6FukHpG-ELxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spatio_textual.utils import save_annotations, load_annotations"
      ],
      "metadata": {
        "id": "S-aclXjeA6lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Annotating spatial entities"
      ],
      "metadata": {
        "id": "COQgoH2iLOpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beyond the typical labels for the named entity recognition task [`PERSON`, `ORG`, `LOC`, `DATE`], we have defined a set of entity labels that are relevant for our work as shown below:\n",
        "\n",
        "| Tag          | Description                                                  |\n",
        "| ------------ | ------------------------------------------------------------ |\n",
        "| `PERSON`     | A named person                                               |\n",
        "| `CONTINENT`  | A continent name (e.g. ‚ÄúEurope‚Äù, ‚ÄúAsia‚Äù)                     |\n",
        "| `COUNTRY`    | A country name (e.g. ‚ÄúGermany‚Äù, ‚ÄúCzechoslovakia‚Äù)            |\n",
        "| `US-STATE`   | A U.S. state name (e.g. ‚ÄúCalifornia‚Äù, ‚ÄúNew York‚Äù)            |\n",
        "| `CITY`       | A city name (e.g. ‚ÄúBerlin‚Äù, ‚ÄúLondon‚Äù,  when classified)     |\n",
        "| `CAMP`       | A Holocaust-camp name e.g. ‚ÄúAuschwitz‚Äù (from your custom list)                |\n",
        "| `PLACE`      | Other place-type entities not matched above                  |\n",
        "| `GEONOUN`    | Generic geographic nouns (e.g. ‚Äúvalley‚Äù, ‚Äúmoor‚Äù)             |\n",
        "| `NON-VERBAL` | Terms like [PAUSES], [LAUGHS] in non-verbal list |\n",
        "| `FAMILY`     | Kinship terms (e.g. ‚Äúmother‚Äù, ‚Äúuncle‚Äù)                       |\n",
        "| `DATE`       | Temporal expressions (e.g. ‚ÄúMarch 9, 1996‚Äù)                  |\n",
        "| `TIME`       | Time-of-day expressions (e.g. ‚Äú3 PM‚Äù)                        |\n",
        "| `EVENT`      | Named events (e.g. ‚ÄúD-Day‚Äù)                                  |\n",
        "| `QUANTITY`   | Numeric/measure expressions (e.g. ‚Äú100 miles‚Äù)               |\n",
        "\n",
        "We will demonstrate how to use the `annotate` module functions to label spatial entities in text in the next cells."
      ],
      "metadata": {
        "id": "ErwJyCwoOn9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotating text: `annotate_text(...)`"
      ],
      "metadata": {
        "id": "ApoODrTs87P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###### Assume we have our text stored in `text` variable as below...\n",
        "text = \"\"\"During the summer of 1942, my family and I were deported from our home in Krakow to the Plaszow labor camp.\n",
        "We spent several difficult months there before being transferred to Auschwitz-Birkenau.\"\n",
        "\"\"\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "NxjAfBllvcoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###### ...we can then identify the entities with the `annotate_text(...)` function.\n",
        "result = annotate_text(text)\n",
        "result"
      ],
      "metadata": {
        "id": "PK-CFbiTsUx6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see tokens identified as places (`PLACE`), geonouns (`GEONOUN`), camps (`CAMP`) etc.\n",
        "\n",
        "Observe, however, that the `verb_data` value is empty. The `verb_data` is meant to capture the 'actions' performed by actors in the text for further analysis.\n",
        "\n",
        "To fix this, we can set the optional parameter `include-verbs` to `True` in the function call."
      ],
      "metadata": {
        "id": "-4ZXu8eS4ZY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###### So let's modify the function calls to extract `verb_data`\n",
        "result = annotate_text(text, include_verbs=True)\n",
        "\n",
        "print(\"Entities:\")\n",
        "display(result['entities'])\n",
        "\n",
        "print(\"\\nVerb Data:\")\n",
        "display(result['verb_data'])"
      ],
      "metadata": {
        "id": "CS4v-HoKWrC0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotating a list of texts"
      ],
      "metadata": {
        "id": "v-NkvcFLE8H2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have a collection of texts (instead of just one piece of text), we can use the `annotate_texts(...)` instead."
      ],
      "metadata": {
        "id": "WaGV2fSNBd7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_texts = [\n",
        "    \"My family and I were deported from our home in Krakow to the Plaszow labor camp.\",\n",
        "    \"We spent several difficult months there before being transferred to Maribor.\",\n",
        "    \"Finally, we arrived at a place with barbed wire fences and watchtowers.\",\n",
        "    \"It was Auschwitz.\"\n",
        "]\n",
        "\n",
        "results = annotate_texts(list_of_texts, include_verbs=True)\n",
        "for result in results:\n",
        "    print(\"\\nEntities:\")\n",
        "    display(result['entities'])\n",
        "\n",
        "    print(\"\\nVerb Data:\")\n",
        "    display(result['verb_data'])"
      ],
      "metadata": {
        "id": "3_pMriYiCTgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotating text segments"
      ],
      "metadata": {
        "id": "OzoBuJOKjsiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, we may need to split a text into segments (or chunks) before annotating them. This is similar to annotating a list of texts only that it includes the segmentation feature.\n",
        "\n",
        "This can be achieved by using `chunk_and_annotate_text(...)` on a text string or using `chunk_and_annotate_file(...)` on a text file. In both cases a key parameter to set is the `n_segments` which specifies the number of segments."
      ],
      "metadata": {
        "id": "obElrU2hj53B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using `chunk_and_annotate_text(...)`"
      ],
      "metadata": {
        "id": "kdUT5Q4Nv4EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### To demonstrate this let's download, read and display the text file `long-text`...\n",
        "!wget -c -q \"https://raw.githubusercontent.com/SpaceTimeNarratives/spatio-textual/refs/heads/main/example-texts/long-text\"\n",
        "text = open(\"long-text\", 'r').read()\n",
        "display(text)"
      ],
      "metadata": {
        "id": "pA_j2RQ0jrzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### ... then let's segment and annotate it with `chunk_and_annotate_text(...)`\n",
        "result = chunk_and_annotate_text(text, n_segments=5, include_text=True)\n",
        "result"
      ],
      "metadata": {
        "id": "P4jygm2Gl4Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using `chunk_and_annotate_file(...)`"
      ],
      "metadata": {
        "id": "T7PtumbGwJW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**NOTE:** You can also upload your file using the file icon in the left sidebar. Then, replace the example text `long-text` with the name of your uploaded file."
      ],
      "metadata": {
        "id": "iY0BiQkrcRVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### ... Let's segment and annotate a file `chunk_and_annotate_file(...)`\n",
        "result = chunk_and_annotate_file('long-text', n_segments=5)\n",
        "result"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NzmT58ASwKP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotating files"
      ],
      "metadata": {
        "id": "ZRduMzA98pBR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "456eaef0"
      },
      "source": [
        "We can also pass a file or a collection of files in a folder as an input to annotate,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = annotate_files('long-text',\n",
        "                        chunk=True,\n",
        "                        n_segments=5,\n",
        "                        include_text=True,\n",
        "                        include_verbs=True)\n",
        "result"
      ],
      "metadata": {
        "id": "v64g-VB0zzdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and loading files"
      ],
      "metadata": {
        "id": "cVkL7vLqBpdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotations can be saved for future analysis and visualisation using the `save_annotations()` function. The supported formats include: `json`, `jsonl`, `csv` and `tsv`."
      ],
      "metadata": {
        "id": "KzWH0kjSB33z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from spatio_textual.utils import save_annotations\n",
        "save_annotations(result, 'result.jsonl')"
      ],
      "metadata": {
        "id": "xENxoDoW1taO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saved annotation can be reloaded in memory as a Pandas dataframe using the `load_annotations()` function."
      ],
      "metadata": {
        "id": "VEPrNHBfFMTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_annotations('result.jsonl')"
      ],
      "metadata": {
        "id": "Pl3eExEtBYps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ToDo\n",
        "Additional features to include\n",
        "- ~Annotating a list of texts~ Done ‚úÖ\n",
        "- ~Saving annotations to files (what format: csv/tsv, json, jsonl)~ Done ‚úÖ\n",
        "- ~loading annotations from corpus file~ Done ‚úÖ\n",
        "- Emotion classification (LLM vs BERT-based)\n",
        "- Sentiment.\n",
        "- Geocoding\n",
        "- Create the `Lake District` and `Holocaust` modules?\n",
        "  - `Holocaust`:\n",
        "      - `journey` extraction\n",
        "      - `event` extraction\n",
        "  - `Lake District`:\n",
        "      - `nearness`\n",
        "      - `wild` and `picturesque`\n",
        "- Analysis\n",
        "- Visualization"
      ],
      "metadata": {
        "id": "Lda1ctcazsUL"
      }
    }
  ]
}